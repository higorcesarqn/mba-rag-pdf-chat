# Sistema RAG - IngestÃ£o e Busca SemÃ¢ntica com LangChain ğŸš€

Sistema completo de **RAG (Retrieval-Augmented Generation)** que permite fazer perguntas sobre documentos PDF usando busca semÃ¢ntica e LLM (Large Language Models).

## ğŸ¯ Funcionalidades

- âœ… **IngestÃ£o de PDF**: Processa arquivos PDF, divide em chunks e armazena embeddings
- âœ… **Busca SemÃ¢ntica**: Busca vetorial usando PostgreSQL + pgVector
- âœ… **Multi-LLM**: Suporta OpenAI e Google Gemini (configurÃ¡vel)
- âœ… **Chat Interativo**: Interface CLI amigÃ¡vel com comandos especiais
- âœ… **Respostas Baseadas em Contexto**: Responde apenas com informaÃ§Ãµes do PDF

## ğŸ› ï¸ Tecnologias

| Categoria          | Tecnologia                  |
| ------------------ | --------------------------- |
| **Linguagem**      | Python 3.13+                |
| **Framework**      | LangChain                   |
| **Banco de Dados** | PostgreSQL 17 + pgVector    |
| **Gerenciador**    | uv (Python package manager) |
| **Container**      | Docker & Docker Compose     |
| **LLM Providers**  | OpenAI ou Google Gemini     |

## ğŸ“‹ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:

- âœ… **Python 3.10+** ([Download](https://www.python.org/downloads/))
- âœ… **Docker Desktop** ([Download](https://www.docker.com/products/docker-desktop/))
- âœ… **uv** - Gerenciador de pacotes Python ([InstalaÃ§Ã£o](https://docs.astral.sh/uv/))
- âœ… **API Key** da OpenAI OU Google Gemini

### Como obter API Keys:

- **OpenAI**: https://platform.openai.com/api-keys
- **Google Gemini**: https://aistudio.google.com/app/apikey

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone <url-do-repositorio>
cd projeto1
```

### 2. Instale as dependÃªncias

**Usando UV (recomendado):**

```bash
uv pip install -r requirements.txt
```

**Ou usando Python/pip padrÃ£o:**

```bash
python -m pip install -r requirements.txt
```

### 3. Configure as variÃ¡veis de ambiente

```bash
# Copiar template
cp .env.example .env

# Editar .env e adicionar suas API keys
# No Windows: notepad .env
# No Linux/Mac: nano .env
```

**ConfiguraÃ§Ã£o mÃ­nima no `.env`:**

```env
# Escolha o provider: "openai" ou "google"
LLM_PROVIDER=google

# Se usar Google Gemini
GOOGLE_API_KEY=sua_chave_aqui

# Se usar OpenAI
# OPENAI_API_KEY=sk-sua_chave_aqui

# Database (jÃ¡ configurado)
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rag
```

### 4. Suba o banco de dados

```bash
docker-compose up -d
```

Aguarde alguns segundos e verifique se estÃ¡ rodando:

```bash
docker ps
```

### 5. Instale a extensÃ£o pgVector (se necessÃ¡rio)

```bash
docker exec postgres_rag psql -U postgres -d rag -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

## ğŸ“– Uso

### Passo 1: Ingerir um PDF

Processe seu PDF e armazene no banco de dados:

**Usando UV:**

```bash
uv run python src/ingest.py seu_arquivo.pdf
uv run python src/ingest.py
uv run python src/ingest.py document.pdf --clear
uv run python src/ingest.py document.pdf --debug
```

**Ou usando Python diretamente:**

```bash
python src/ingest.py seu_arquivo.pdf
python src/ingest.py
python src/ingest.py document.pdf --clear
python src/ingest.py document.pdf --debug
```

**SaÃ­da esperada:**

```
ğŸš€ Inicializando PDFIngestionService
ğŸ“¡ Provider: GOOGLE
âœ… PDF carregado: 15 pÃ¡gina(s)
âœ… Documento dividido em 42 chunk(s)
âœ… Embeddings gerados e salvos com sucesso!
âœ… INGESTÃƒO CONCLUÃDA COM SUCESSO!
```

### Passo 2: Fazer perguntas (Chat Interativo)

**Usando UV:**

```bash
uv run python src/chat.py
uv run python src/chat.py --query "Qual o assunto principal?"
uv run python src/chat.py --debug
```

**Ou usando Python diretamente:**

```bash
python src/chat.py
python src/chat.py --query "Qual o assunto principal?"
python src/chat.py --debug
```

**Exemplo de interaÃ§Ã£o:**

```
ğŸ’¬ CHAT COM PDF - RAG SYSTEM
ğŸ¤– Provider: GOOGLE
ğŸ’¡ Dicas: Digite 'help' para comandos especiais
============================================================

ğŸ’¬ Sua pergunta: Qual o faturamento da empresa?

â³ Processando...

============================================================
ğŸ’¬ RESPOSTA:
============================================================
O faturamento foi de 10 milhÃµes de reais.
============================================================

ğŸ’¬ Sua pergunta: Quantos clientes temos em 2024?

â³ Processando...

============================================================
ğŸ’¬ RESPOSTA:
============================================================
NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
============================================================

ğŸ’¬ Sua pergunta: sair
ğŸ‘‹ Encerrando chat. AtÃ© logo!
```

### Comandos do Chat

| Comando | DescriÃ§Ã£o              |
| ------- | ---------------------- |
| `help`  | Mostra ajuda           |
| `info`  | InformaÃ§Ãµes do sistema |
| `clear` | Limpa a tela           |
| `sair`  | Encerra o chat         |

## âš™ï¸ ConfiguraÃ§Ã£o Multi-LLM

O sistema suporta dois provedores de LLM. Edite o `.env` para trocar:

### OpÃ§Ã£o 1: OpenAI (GPT)

```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-sua_chave_aqui
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_CHAT_MODEL=gpt-4o-mini
```

### OpÃ§Ã£o 2: Google Gemini

```env
LLM_PROVIDER=google
GOOGLE_API_KEY=AIza_sua_chave_aqui
GOOGLE_EMBEDDING_MODEL=models/embedding-001
GOOGLE_CHAT_MODEL=gemini-2.0-flash-exp
```

## ğŸ“ Estrutura do Projeto

```
projeto1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ llm_factory.py         # Factory para criar LLMs
â”‚   â”œâ”€â”€ ingest.py              # Script de ingestÃ£o de PDF
â”‚   â”œâ”€â”€ search.py              # ServiÃ§o de busca semÃ¢ntica
â”‚   â”œâ”€â”€ chat.py                # Interface CLI interativa
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py          # Sistema de logging
â”‚       â””â”€â”€ database.py        # UtilitÃ¡rios de banco
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_llm_factory.py    # Testes do factory de LLM
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ test_database.py   # Testes de utilitÃ¡rios de banco
â”œâ”€â”€ pytest.ini                 # ConfiguraÃ§Ã£o do pytest (pythonpath/tests)
â”œâ”€â”€ docker-compose.yml         # PostgreSQL + pgVector
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â”œâ”€â”€ .env.example               # Template de configuraÃ§Ã£o
â”œâ”€â”€ .env                       # ConfiguraÃ§Ã£o (nÃ£o versionar)
â””â”€â”€ README.md                  # Este arquivo
```

## ğŸ”§ ParÃ¢metros ConfigurÃ¡veis

Todas as configuraÃ§Ãµes estÃ£o no arquivo `.env`:

| VariÃ¡vel        | DescriÃ§Ã£o                              | PadrÃ£o   |
| --------------- | -------------------------------------- | -------- |
| `LLM_PROVIDER`  | Provider de LLM (`openai` ou `google`) | `openai` |
| `CHUNK_SIZE`    | Tamanho dos chunks em caracteres       | `1000`   |
| `CHUNK_OVERLAP` | SobreposiÃ§Ã£o entre chunks              | `150`    |
| `SEARCH_K`      | NÃºmero de documentos similares         | `10`     |
| `LOG_LEVEL`     | NÃ­vel de log (DEBUG, INFO, etc.)       | `ERROR`  |

## ğŸ§ª Testes

Os testes automatizados vivem em `tests/` e utilizam `pytest`.

### Instalar dependÃªncias de desenvolvimento

**Usando UV (recomendado):**

```bash
uv sync
```

**Ou sincronizando via requirements.txt:**

```bash
uv pip install -r requirements.txt
```

### Executar a suÃ­te completa

```bash
uv run pytest
```

### Executar com relatÃ³rio de cobertura (opcional)

```bash
uv run pytest --cov=src --cov-report=term-missing
```

### Ajuste manual de PYTHONPATH (apenas se ignorar o pytest.ini)

```bash
# Linux/macOS
PYTHONPATH=src uv run pytest -q

# Windows (PowerShell)
$env:PYTHONPATH="src"; uv run pytest -q
```

## ğŸ› Troubleshooting

### Erro: "No module named 'dotenv'"

**Usando UV:**

```bash
uv pip install -r requirements.txt
uv sync
```

**Ou usando Python/pip padrÃ£o:**

```bash
python -m pip install -r requirements.txt
```

### Erro: "pgvector extension not found"

```bash
docker exec postgres_rag psql -U postgres -d rag -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### Erro: "API key not configured"

Verifique se o arquivo `.env` tem a chave correta:

```bash
cat .env  # Linux/Mac
type .env # Windows
```

### Banco de dados nÃ£o conecta

```bash
# Reiniciar containers
docker-compose down
docker-compose up -d

# Ver logs
docker-compose logs postgres
```

### Collection vazia (sem documentos)

Execute a ingestÃ£o primeiro:

**Usando UV:**

```bash
uv run python src/ingest.py seu_arquivo.pdf
```

**Ou usando Python diretamente:**

```bash
python src/ingest.py seu_arquivo.pdf
```

## ğŸ“š Pacotes Principais

| Pacote                   | VersÃ£o | Uso                     |
| ------------------------ | ------ | ----------------------- |
| `langchain`              | 0.3.27 | Framework principal     |
| `langchain-openai`       | 0.3.30 | IntegraÃ§Ã£o OpenAI       |
| `langchain-google-genai` | 2.1.9  | IntegraÃ§Ã£o Google       |
| `langchain-postgres`     | 0.0.15 | Vector store PostgreSQL |
| `pypdf`                  | 6.0.0  | Leitura de PDF          |
| `psycopg2-binary`        | 2.9.10 | Driver PostgreSQL       |
| `pgvector`               | 0.3.6  | ExtensÃ£o de vetores     |

## ğŸ“ Conceitos Implementados

- **RAG (Retrieval-Augmented Generation)**: Combina busca semÃ¢ntica com LLM
- **Vector Database**: Armazenamento eficiente de embeddings
- **Semantic Search**: Busca por similaridade vetorial
- **Factory Pattern**: CriaÃ§Ã£o de objetos LLM de forma flexÃ­vel
- **Dependency Injection**: ConfiguraÃ§Ã£o centralizada
- **Async/Await**: OperaÃ§Ãµes assÃ­ncronas para melhor performance

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins educacionais para o Desafio MBA Engenharia de Software com IA - Full Cycle.

## ğŸ“ Suporte

Se encontrar problemas:

1. Verifique a seÃ§Ã£o [Troubleshooting](#-troubleshooting)
2. Use `--debug` para ver logs detalhados
3. Verifique os logs do Docker: `docker-compose logs`

---

**Desenvolvido com â¤ï¸ usando Python, LangChain, PostgreSQL e muita IA**

## Requisitos

### 1. IngestÃ£o do PDF

- O PDF deve ser dividido em chunks de 1000 caracteres com overlap de 150.
- Cada chunk deve ser convertido em embedding.
- Os vetores devem ser armazenados no banco de dados PostgreSQL com pgVector.

### 2. Consulta via CLI

- Criar um script Python para simular um chat no terminal.
- Passos ao receber uma pergunta:
  1. Vetorizar a pergunta.
  2. Buscar os 10 resultados mais relevantes (k=10) no banco vetorial.
  3. Montar o prompt e chamar a LLM.
  4. Retornar a resposta ao usuÃ¡rio.

Prompt a ser utilizado:

```
CONTEXTO:
{resultados concatenados do banco de dados}

REGRAS:
- Responda somente com base no CONTEXTO.
- Se a informaÃ§Ã£o nÃ£o estiver explicitamente no CONTEXTO, responda:
  "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo.
- Nunca produza opiniÃµes ou interpretaÃ§Ãµes alÃ©m do que estÃ¡ escrito.

EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:
Pergunta: "Qual Ã© a capital da FranÃ§a?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "Quantos clientes temos em 2024?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "VocÃª acha isso bom ou ruim?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

PERGUNTA DO USUÃRIO:
{pergunta do usuÃ¡rio}

RESPONDA A "PERGUNTA DO USUÃRIO"
```

## Estrutura obrigatÃ³ria do projeto

```
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt      # DependÃªncias
â”œâ”€â”€ .env.example          # Template da variÃ¡vel OPENAI_API_KEY
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py         # Script de ingestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py         # Script de busca
â”‚   â”œâ”€â”€ chat.py           # CLI para interaÃ§Ã£o com usuÃ¡rio
â”œâ”€â”€ document.pdf          # PDF para ingestÃ£o
â””â”€â”€ README.md             # InstruÃ§Ãµes de execuÃ§Ã£o
```
