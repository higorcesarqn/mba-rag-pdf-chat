"""
Factory Pattern para cria√ß√£o de inst√¢ncias de LLM e Embeddings.

centraliza a cria√ß√£o de objetos baseado em configura√ß√£o, 
permitindo trocar facilmente entre providers
(OpenAI ou Google Gemini) sem modificar c√≥digo cliente.

Exemplo de uso:
    ```python
    from llm_factory import LLMFactory
    
    # Criar apenas embeddings
    embeddings = LLMFactory.create_embeddings()
    
    # Criar apenas chat model
    chat_model = LLMFactory.create_chat_model()
    
    # Criar ambos de uma vez
    embeddings, chat_model = LLMFactory.create_all()
    ```
"""

from typing import Tuple
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.embeddings.base import Embeddings
from langchain.chat_models.base import BaseChatModel

from config import Config
from utils.logger import setup_logger

logger = setup_logger(__name__)


class LLMFactory:
    """
    Factory para criar inst√¢ncias de LLM e Embeddings.
    
    Attributes:
        Nenhum (apenas m√©todos est√°ticos)
    """
    
    @staticmethod
    def create_embeddings() -> Embeddings:
        """
        Cria inst√¢ncia de embeddings baseado no provider configurado.
        
        Returns:
            Inst√¢ncia de Embeddings (OpenAI ou Google)
            
        Raises:
            ValueError: Se provider n√£o for suportado
            
        Exemplo:
            ```python
            embeddings = LLMFactory.create_embeddings()
            vectors = embeddings.embed_documents(["texto exemplo"])
            ```
        """
        provider = Config.LLM_PROVIDER
        
        if provider == "openai":
            logger.info(f"ü§ñ Inicializando OpenAI Embeddings: {Config.OPENAI_EMBEDDING_MODEL}")
            
            return OpenAIEmbeddings(
                model=Config.OPENAI_EMBEDDING_MODEL,
                api_key=Config.OPENAI_API_KEY
            )
        
        elif provider == "google":
            logger.info(f"ü§ñ Inicializando Google Embeddings: {Config.GOOGLE_EMBEDDING_MODEL}")
            
            return GoogleGenerativeAIEmbeddings(
                model=Config.GOOGLE_EMBEDDING_MODEL,
                google_api_key=Config.GOOGLE_API_KEY
            )
        
        else:
            error_msg = f"Provider n√£o suportado: {provider}. Use 'openai' ou 'google'"
            logger.error(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
    
    @staticmethod
    def create_chat_model(temperature: float = 0.0) -> BaseChatModel:
        """
        Cria inst√¢ncia de chat model baseado no provider configurado.
        
        Args:
            temperature: Temperatura para gera√ß√£o (0.0 = determin√≠stico, 1.0 = criativo)
                        Default: 0.0 para respostas consistentes
        
        Returns:
            Inst√¢ncia de ChatModel (OpenAI ou Google)
            
        Raises:
            ValueError: Se provider n√£o for suportado
            
        Exemplo:
            ```python
            chat = LLMFactory.create_chat_model()
            response = chat.invoke("Ol√°, como voc√™ est√°?")
            ```
        """
        provider = Config.LLM_PROVIDER
        
        if provider == "openai":
            logger.info(f"ü§ñ Inicializando OpenAI Chat: {Config.OPENAI_CHAT_MODEL}")
            
            return ChatOpenAI(
                model=Config.OPENAI_CHAT_MODEL,
                api_key=Config.OPENAI_API_KEY,
                temperature=temperature
            )
        
        elif provider == "google":
            logger.info(f"ü§ñ Inicializando Google Chat: {Config.GOOGLE_CHAT_MODEL}")
            
            return ChatGoogleGenerativeAI(
                model=Config.GOOGLE_CHAT_MODEL,
                google_api_key=Config.GOOGLE_API_KEY,
                temperature=temperature
            )
        
        else:
            error_msg = f"Provider n√£o suportado: {provider}. Use 'openai' ou 'google'"
            logger.error(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
    
    @staticmethod
    def create_all(temperature: float = 0.0) -> Tuple[Embeddings, BaseChatModel]:
        """
        Cria embeddings e chat model de uma vez.
        
        √ötil quando voc√™ precisa de ambos (como no SearchService).
        
        Args:
            temperature: Temperatura para o chat model
        
        Returns:
            Tupla (embeddings, chat_model)
            
        Exemplo:
            ```python
            embeddings, chat = LLMFactory.create_all()
            
            # Usar embeddings para busca
            vectors = embeddings.embed_query("minha pergunta")
            
            # Usar chat para gerar resposta
            response = chat.invoke("responda isso")
            ```
        """
        logger.info(f"üè≠ Factory: Criando embeddings e chat model ({Config.LLM_PROVIDER})")
        
        embeddings = LLMFactory.create_embeddings()
        chat_model = LLMFactory.create_chat_model(temperature)
        
        logger.info("‚úÖ Factory: Inst√¢ncias criadas com sucesso")
        
        return embeddings, chat_model
    
    @staticmethod
    def get_provider_info() -> dict:
        """
        Retorna informa√ß√µes sobre o provider atual.
        
        Returns:
            Dicion√°rio com informa√ß√µes do provider
        """
        provider = Config.LLM_PROVIDER
        
        if provider == "openai":
            return {
                "provider": "OpenAI",
                "embedding_model": Config.OPENAI_EMBEDDING_MODEL,
                "chat_model": Config.OPENAI_CHAT_MODEL,
                "api_key_set": bool(Config.OPENAI_API_KEY)
            }
        elif provider == "google":
            return {
                "provider": "Google Gemini",
                "embedding_model": Config.GOOGLE_EMBEDDING_MODEL,
                "chat_model": Config.GOOGLE_CHAT_MODEL,
                "api_key_set": bool(Config.GOOGLE_API_KEY)
            }
        else:
            return {
                "provider": "Unknown",
                "error": f"Provider inv√°lido: {provider}"
            }


if __name__ == "__main__":
    """Teste standalone do factory"""
    
    print("\n" + "=" * 60)
    print("üß™ TESTANDO LLM FACTORY")
    print("=" * 60 + "\n")
    
    # Mostrar configura√ß√£o
    Config.display_config()
    
    print("\nüìä Informa√ß√µes do Provider:")
    info = LLMFactory.get_provider_info()
    for key, value in info.items():
        print(f"   - {key}: {value}")
    
    print("\nüèóÔ∏è  Testando cria√ß√£o de inst√¢ncias...\n")
    
    try:
        # Testar embeddings
        print("1Ô∏è‚É£  Criando Embeddings...")
        embeddings = LLMFactory.create_embeddings()
        print(f"   ‚úÖ Tipo: {type(embeddings).__name__}\n")
        
        # Testar chat model
        print("2Ô∏è‚É£  Criando Chat Model...")
        chat = LLMFactory.create_chat_model()
        print(f"   ‚úÖ Tipo: {type(chat).__name__}\n")
        
        # Testar create_all
        print("3Ô∏è‚É£  Criando ambos (create_all)...")
        emb, ch = LLMFactory.create_all()
        print(f"   ‚úÖ Embeddings: {type(emb).__name__}")
        print(f"   ‚úÖ Chat: {type(ch).__name__}\n")
        
        print("=" * 60)
        print("‚úÖ TODOS OS TESTES PASSARAM!")
        print("=" * 60 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå ERRO: {str(e)}\n")
        logger.error("Erro no teste do factory", exc_info=True)
